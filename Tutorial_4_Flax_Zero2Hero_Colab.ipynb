{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 1: Flax Zero2Hero.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxLZLcO0/SkVgWClaxNn8V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gordicaleksa/get-started-with-JAX/blob/main/Tutorial_4_Flax_Zero2Hero_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbMr3-5oun69"
      },
      "source": [
        "# Warming up with Flax\n",
        "\n",
        "This notebook heavily relies on https://flax.readthedocs.io/en/latest/ + some additional code/modifications, comments/notes, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1qve53yeof5"
      },
      "source": [
        "### Flax basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHcasJkggdZN"
      },
      "source": [
        "# Install Flax\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmVx7EjigrEZ"
      },
      "source": [
        "import jax\n",
        "from jax import lax, random, numpy as jnp\n",
        "\n",
        "import flax\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn  # nn is also used in PyTorch and in older Flax API\n",
        "\n",
        "import optax  # JAX optimizers\n",
        "\n",
        "from typing import Any, Callable, Sequence, Optional\n",
        "import functools\n",
        "import numpy as np"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1kdq0P_g7LU",
        "outputId": "ac818156-d698-460a-e371-577ec344ccd2"
      },
      "source": [
        "model = nn.Dense(features=5)\n",
        "\n",
        "# Dense as well as all other layers inherit from Module class (same as in PyTorch...)\n",
        "print(nn.Dense.__bases__)\n",
        "\n",
        "# todo: check the source code: https://github.com/google/flax/blob/main/flax/linen/linear.py"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<class 'flax.linen.module.Module'>,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QViTvJhFite2",
        "outputId": "46de8191-65b0-4107-89b0-f3b3d3132951"
      },
      "source": [
        "seed = 0\n",
        "key1, key2 = random.split(random.PRNGKey(seed))\n",
        "\n",
        "x = random.normal(key1, (10,))  # Dummy input\n",
        "params = model.init(key2, x)  # Initialization call\n",
        "jax.tree_map(lambda x: x.shape, params)  # Checking output shapes\n",
        "\n",
        "# Note1: automatic shape inference\n",
        "# Note2: immutable structure\n",
        "# Note2: init_with_output"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    params: {\n",
              "        bias: (5,),\n",
              "        kernel: (10, 5),\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3yFAqeTjdLj"
      },
      "source": [
        "# This is how you run prediction in Flax\n",
        "model.apply(params, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31O_mx-Smalq"
      },
      "source": [
        "try:\n",
        "    y = model(x) # Returns an error\n",
        "except AttributeError as e:\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUBYtd40krx1"
      },
      "source": [
        "### Small example - linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53-TXcbYkt9D"
      },
      "source": [
        "# Set problem dimensions\n",
        "nsamples = 20\n",
        "xdim = 10\n",
        "ydim = 5\n",
        "\n",
        "# Generate random ground truth W and b\n",
        "key = random.PRNGKey(0)\n",
        "k1, k2 = random.split(key)\n",
        "W = random.normal(k1, (xdim, ydim))\n",
        "b = random.normal(k2, (ydim,))\n",
        "true_params = freeze({'params': {'bias': b, 'kernel': W}})\n",
        "\n",
        "# Generate samples with additional noise\n",
        "ksample, knoise = random.split(k1)\n",
        "x_samples = random.normal(ksample, (nsamples, xdim))\n",
        "y_samples = jnp.dot(x_samples, W) + b \n",
        "y_samples += 0.1*random.normal(knoise,(nsamples, ydim)) # Adding noise\n",
        "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKiCOyoikxcM"
      },
      "source": [
        "def make_mse_func(x_batched, y_batched):\n",
        "  def mse(params):\n",
        "    # Define the squared loss for a single pair (x,y)\n",
        "    def squared_error(x, y):\n",
        "      pred = model.apply(params, x)\n",
        "      return jnp.inner(y-pred, y-pred)/2.0\n",
        "    # We vectorize the previous to compute the average of the loss on all samples.\n",
        "    return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
        "  return jax.jit(mse) # And finally we jit the result.\n",
        "\n",
        "# Get the sampled loss\n",
        "loss = make_mse_func(x_samples, y_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phLYjH5ZkzLn"
      },
      "source": [
        "alpha = 0.3 # Gradient step size\n",
        "print('Loss for \"true\" W,b: ', loss(true_params))\n",
        "grad_fn = jax.value_and_grad(loss)\n",
        "\n",
        "for i in range(101):\n",
        "  # We perform one gradient update\n",
        "  loss_val, grads = grad_fn(params)\n",
        "  params = jax.tree_multimap(lambda p, g: p - alpha * g,\n",
        "                            params, grads)\n",
        "  if i % 10 == 0:\n",
        "    print('Loss step {}: '.format(i), loss_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvy6Oow2lLHu"
      },
      "source": [
        "Doing the same thing with dedicated optimizers!\n",
        "\n",
        "Enter DeepMind's optax!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hhcFZ7UlCov"
      },
      "source": [
        "tx = optax.sgd(learning_rate=alpha)\n",
        "opt_state = tx.init(params)\n",
        "loss_grad_fn = jax.value_and_grad(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_EHHjy_lFGN"
      },
      "source": [
        "for i in range(101):\n",
        "  loss_val, grads = loss_grad_fn(params)\n",
        "  updates, opt_state = tx.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  if i % 10 == 0:\n",
        "    print('Loss step {}: '.format(i), loss_val)\n",
        "\n",
        "# After training you can save/load the model using flax's serialization module "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_33y-bTl6bd"
      },
      "source": [
        "### Create custom models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOrJHqTSl75M"
      },
      "source": [
        "class ExplicitMLP(nn.Module):\n",
        "  features: Sequence[int]  # data field (Module is Python's dataclass)\n",
        "\n",
        "  def setup(self):  # because Python dataclass took the __init__ function...\n",
        "    # we automatically know what to do with lists, dicts of submodules\n",
        "    self.layers = [nn.Dense(feat) for feat in self.features]\n",
        "    # for single submodules, we would just write:\n",
        "    # self.layer1 = nn.Dense(feat1)\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    x = inputs\n",
        "    for i, lyr in enumerate(self.layers):\n",
        "      x = lyr(x)\n",
        "      if i != len(self.layers) - 1:\n",
        "        x = nn.relu(x)\n",
        "    return x\n",
        "\n",
        "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
        "x = random.uniform(key1, (4,4))\n",
        "\n",
        "model = ExplicitMLP(features=[3,4,5])\n",
        "params = model.init(key2, x)\n",
        "y = model.apply(params, x)\n",
        "\n",
        "print('initialized parameter shapes:\\n', jax.tree_map(jnp.shape, unfreeze(params)))\n",
        "print('output:\\n', y)\n",
        "\n",
        "# todo: use @nn.compact instead"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEhC-WdPnAYp"
      },
      "source": [
        "Going deeper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9YhSgxjnBQg"
      },
      "source": [
        "class SimpleDense(nn.Module):\n",
        "  features: int\n",
        "  kernel_init: Callable = nn.initializers.lecun_normal()\n",
        "  bias_init: Callable = nn.initializers.zeros\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "      # We could also declare kernel/bias in setup() fn\n",
        "    kernel = self.param('kernel',\n",
        "                        self.kernel_init, # Initialization function, RNG passed implicitly\n",
        "                        (inputs.shape[-1], self.features))  # shape info.\n",
        "    y = lax.dot_general(inputs, kernel,\n",
        "                        (((inputs.ndim - 1,), (0,)), ((), ())),) # TODO Why not jnp.dot?\n",
        "    bias = self.param('bias', self.bias_init, (self.features,))\n",
        "    y = y + bias\n",
        "    return y\n",
        "\n",
        "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
        "x = random.uniform(key1, (4,4))\n",
        "\n",
        "model = SimpleDense(features=3)\n",
        "params = model.init(key2, x)\n",
        "y = model.apply(params, x)\n",
        "\n",
        "print('initialized parameters:\\n', params)\n",
        "print('output:\\n', y)\n",
        "\n",
        "\n",
        "from inspect import signature\n",
        "print(signature(nn.initializers.lecun_normal()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWB8HvLHn6g0"
      },
      "source": [
        "Introducing state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGE6qTHHngYh"
      },
      "source": [
        "class BiasAdderWithRunningMean(nn.Module):\n",
        "  decay: float = 0.99\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    # easy pattern to detect if we're initializing via empty variable tree\n",
        "    is_initialized = self.has_variable('batch_stats', 'mean')\n",
        "    ra_mean = self.variable('batch_stats', 'mean', lambda s: jnp.zeros(s), x.shape[1:])\n",
        "    # self.param will by default add this variable to 'params' collection\n",
        "    # self.variable returns a reference hence .value\n",
        "    bias = self.param('bias', lambda rng, shape: jnp.zeros(shape), x.shape[1:])\n",
        "    if is_initialized:\n",
        "      ra_mean.value = self.decay * ra_mean.value + (1.0 - self.decay) * jnp.mean(x, axis=0, keepdims=True)\n",
        "\n",
        "    return x - ra_mean.value + bias\n",
        "\n",
        "\n",
        "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
        "x = jnp.ones((10,5))\n",
        "model = BiasAdderWithRunningMean()\n",
        "variables = model.init(key1, x)\n",
        "print('initialized variables:\\n', variables)\n",
        "y, updated_state = model.apply(variables, x, mutable=['batch_stats'])\n",
        "print('updated state:\\n', updated_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBw5TGuboD5_"
      },
      "source": [
        "for val in [1.0, 2.0, 3.0]:\n",
        "  x = val * jnp.ones((10,5))\n",
        "  y, updated_state = model.apply(variables, x, mutable=['batch_stats'])\n",
        "  old_state, params = variables.pop('params')\n",
        "  variables = freeze({'params': params, **updated_state})\n",
        "  print('updated state:\\n', updated_state) # Shows only the mutable part"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PioD28UoZiv"
      },
      "source": [
        "Adding the optimizer (maybe delete the above cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuzwVt8RoHvY"
      },
      "source": [
        "def update_step(tx, apply_fn, x, opt_state, params, state):\n",
        "\n",
        "  def loss(params):\n",
        "    y, updated_state = apply_fn({'params': params, **state},\n",
        "                                x, mutable=list(state.keys()))\n",
        "    l = ((x - y) ** 2).sum()\n",
        "    return l, updated_state\n",
        "\n",
        "  (l, state), grads = jax.value_and_grad(loss, has_aux=True)(params)\n",
        "  updates, opt_state = tx.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return opt_state, params, state\n",
        "\n",
        "x = jnp.ones((10,5))\n",
        "variables = model.init(random.PRNGKey(0), x)\n",
        "state, params = variables.pop('params')\n",
        "del variables  # Delete variables to avoid wasting resources\n",
        "tx = optax.sgd(learning_rate=0.02)\n",
        "opt_state = tx.init(params)\n",
        "\n",
        "for _ in range(3):\n",
        "  opt_state, params, state = update_step(tx, model.apply, x, opt_state, params, state)\n",
        "  print('Updated state: ', state)\n",
        "\n",
        "# todo: flax.training.train_state.TrainState\n",
        "# todo: BatchNorm ('batch_stats' collection added) see source code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzWUq5vBrWMe"
      },
      "source": [
        "More comprehensive, contrived example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDw2986orY0a"
      },
      "source": [
        "class Block(nn.Module):\n",
        "  features: int\n",
        "  training: bool\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    x = nn.Dense(self.features)(inputs)\n",
        "    x = nn.Dropout(rate=0.5)(x, deterministic=not self.training)\n",
        "    x = nn.BatchNorm(use_running_average=not self.training)(x)\n",
        "    return x\n",
        "\n",
        "key1, key2, key3, key4 = random.split(random.PRNGKey(0), 4)\n",
        "x = random.uniform(key1, (3,4,4))\n",
        "\n",
        "model = Block(features=3, training=True)\n",
        "\n",
        "init_variables = model.init({'params': key2, 'dropout': key3}, x)\n",
        "_, init_params = init_variables.pop('params')\n",
        "\n",
        "# When calling `apply` with mutable kinds, returns a pair of output, \n",
        "# mutated_variables.\n",
        "y, mutated_variables = model.apply(\n",
        "    init_variables, x, rngs={'dropout': key4}, mutable=['batch_stats'])\n",
        "\n",
        "# Now we reassemble the full variables from the updates (in a real training\n",
        "# loop, with the updated params from an optimizer).\n",
        "updated_variables = freeze(dict(params=init_params, \n",
        "                                **mutated_variables))\n",
        "\n",
        "print('updated variables:\\n', updated_variables)\n",
        "print('initialized variable shapes:\\n', \n",
        "      jax.tree_map(jnp.shape, init_variables))\n",
        "print('output:\\n', y)\n",
        "\n",
        "# Let's run these model variables during \"evaluation\":\n",
        "eval_model = Block(features=3, training=False)\n",
        "y = eval_model.apply(updated_variables, x)  # Nothing mutable; single return value.\n",
        "print('eval output:\\n', y)\n",
        "\n",
        "# check out remat if you have memory expensive computation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB2SnZvnvLqe"
      },
      "source": [
        "# capture_intermediates\n",
        "# check out https://flax.readthedocs.io/en/latest/howtos/extracting_intermediates.html\n",
        "# for more options\n",
        "\n",
        "# todo: add this to some of the above examples instead\n",
        "@jax.jit\n",
        "def init(key, x):\n",
        "  variables = CNN().init(key, x)\n",
        "  return variables\n",
        "\n",
        "@jax.jit\n",
        "def predict(variables, x):\n",
        "  y, state = CNN().apply(variables, x, capture_intermediates=True, mutable=[\"intermediates\"])\n",
        "  intermediates = state['intermediates']\n",
        "  fin = jax.tree_map(lambda xs: jnp.all(jnp.isfinite(xs)), intermediates)\n",
        "  return y, fin\n",
        "\n",
        "variables = init(jax.random.PRNGKey(0), batch)\n",
        "y, is_finite = predict(variables, batch)\n",
        "all_finite = all(jax.tree_leaves(is_finite))\n",
        "assert all_finite, \"non finite intermediate detected!\"\n",
        "\n",
        "filter_Dense = lambda mdl, method_name: isinstance(mdl, nn.Dense)\n",
        "filter_encodings = lambda mdl, method_name: method_name == \"encode\"\n",
        "\n",
        "y, state = CNN().apply(variables, batch, capture_intermediates=filter_Dense, mutable=[\"intermediates\"])\n",
        "dense_intermediates = state['intermediates']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys1y-yM8vzT8"
      },
      "source": [
        "### Full MNIST example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD8t9K2Nv0yC"
      },
      "source": [
        "# 1) annotated\n",
        "# 2) interactive\n",
        "# 3) source code itself\n",
        "# combine the above 3 here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U-BIjQ1v4ff"
      },
      "source": [
        "### Bonus: going through real ImageNet CNN example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbFfwhjZv7nY"
      },
      "source": [
        "# jump to https://github.com/google/flax/tree/main/examples/imagenet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q4C2M2tv_0J"
      },
      "source": [
        "HuggingFace examples, community week are a good resource:\n",
        "\n",
        "1) https://github.com/huggingface/transformers/tree/master/examples/flax\n",
        "\n",
        "2) https://github.com/huggingface/transformers/tree/master/examples/research_projects/jax-projects\n",
        "\n",
        "Source code is also your friend, library is still evolving"
      ]
    }
  ]
}